index: hide
name: Disorder in a Gas

The fantastic growth in the odds favoring disorder that we see in going from 5 to 100 coins continues as the number of entities in the system increases. Let us now imagine applying this approach to perhaps a small sample of gas. Because counting microstates and macrostates involves statistics, this is called  **statistical analysis**. The macrostates of a gas correspond to its macroscopic properties, such as volume, temperature, and pressure; and its microstates correspond to the detailed description of the positions and velocities of its atoms. Even a small amount of gas has a huge number of atoms: <math xmlns:q="http://cnx.rice.edu/qml/1.0" xmlns:fo="urn:oasis:names:tc:opendocument:xmlns:xsl-fo-compatible:1.0" xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:md="http://cnx.rice.edu/mdml" xmlns="http://cnx.rice.edu/cnxml"><semantics><mrow><mrow><mrow><mn>1</mn><mtext>.</mtext><mn>0</mn><msup><mtext> cm</mtext><mrow><mn>3</mn></mrow></msup></mrow></mrow><mrow/></mrow><annotation encoding="StarMath 5.0"> size 12&#123;1 &quot;.&quot; 0&quot; cm&quot; rSup &#123; size 8&#123;3&#125; &#125; &#125; &#123;&#125;</annotation></semantics></math> of an ideal gas at 1.0 atm and <math xmlns:q="http://cnx.rice.edu/qml/1.0" xmlns:fo="urn:oasis:names:tc:opendocument:xmlns:xsl-fo-compatible:1.0" xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:md="http://cnx.rice.edu/mdml" xmlns="http://cnx.rice.edu/cnxml"><semantics><mrow><mrow><mrow><mn>0º C</mn></mrow></mrow><mrow/></mrow><annotation encoding="StarMath 5.0"> size 12&#123;0°C&#125; &#123;&#125;</annotation></semantics></math> has <math xmlns:q="http://cnx.rice.edu/qml/1.0" xmlns:fo="urn:oasis:names:tc:opendocument:xmlns:xsl-fo-compatible:1.0" xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:md="http://cnx.rice.edu/mdml" xmlns="http://cnx.rice.edu/cnxml"><semantics><mrow><mrow><mrow><mn>2</mn><mtext>.</mtext><mrow><mn>7</mn><mo stretchy="false">×</mo><msup><mtext>10</mtext><mrow><mtext>19</mtext></mrow></msup></mrow></mrow></mrow><mrow/></mrow><annotation encoding="StarMath 5.0"> size 12&#123;2 &quot;.&quot; 7 times &quot;10&quot; rSup &#123; size 8&#123;&quot;19&quot;&#125; &#125; &#125; &#123;&#125;</annotation></semantics></math> atoms. So each macrostate has an immense number of microstates. In plain language, this means that there are an immense number of ways in which the atoms in a gas can be arranged, while still having the same pressure, temperature, and so on.

The most likely conditions (or macrostates) for a gas are those we see all the time—a random distribution of atoms in space with a Maxwell-Boltzmann distribution of speeds in random directions, as predicted by kinetic theory. This is the most disorderly and least structured condition we can imagine. In contrast, one type of very orderly and structured macrostate has all of the atoms in one corner of a container with identical velocities. There are very few ways to accomplish this (very few microstates corresponding to it), and so it is exceedingly unlikely ever to occur. (See {'Figure_16_7_2 Figure 16.7.2}(b).) Indeed, it is so unlikely that we have a law saying that it is impossible, which has never been observed to be violated—the second law of thermodynamics.


{image:'Figure_16_7_2}
        

The disordered condition is one of high entropy, and the ordered one has low entropy. With a transfer of energy from another system, we could force all of the atoms into one corner and have a local decrease in entropy, but at the cost of an overall increase in entropy of the universe. If the atoms start out in one corner, they will quickly disperse and become uniformly distributed and will never return to the orderly original state ({'Figure_16_7_2 Figure 16.7.2}(b)). Entropy will increase. With such a large sample of atoms, it is possible—but unimaginably unlikely—for entropy to decrease. Disorder is vastly more likely than order.

The arguments that disorder and high entropy are the most probable states are quite convincing. The great Austrian physicist Ludwig Boltzmann (1844–1906)—who, along with Maxwell, made so many contributions to kinetic theory—proved that the entropy of a system in a given state (a macrostate) can be written as

<math xmlns:fo="urn:oasis:names:tc:opendocument:xmlns:xsl-fo-compatible:1.0" xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:md="http://cnx.rice.edu/mdml" xmlns="http://cnx.rice.edu/cnxml"><semantics><mrow><mml:mrow xmlns:mml="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mrow><mi>S</mi><mo stretchy="false">=</mo><mi>k</mi></mrow><mtext> ln</mtext><mi>W</mi></mrow></mrow><mrow><mtext>,</mtext></mrow></mml:mrow><mrow/></mrow><annotation encoding="StarMath 5.0"> size 12&#123;S=k&quot; ln&quot;W&#125; &#123;&#125;</annotation></semantics></math>

where <math xmlns:q="http://cnx.rice.edu/qml/1.0" xmlns:fo="urn:oasis:names:tc:opendocument:xmlns:xsl-fo-compatible:1.0" xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:md="http://cnx.rice.edu/mdml" xmlns="http://cnx.rice.edu/cnxml"><semantics><mrow><mrow><mrow><mrow><mi>k</mi><mo stretchy="false">=</mo><mn>1</mn></mrow><mtext>.</mtext><mrow><mtext>38</mtext><mo stretchy="false">×</mo><msup><mtext>10</mtext><mrow><mrow><mo stretchy="false">−</mo><mtext>23</mtext></mrow></mrow></msup></mrow><mspace width="0.25em"/><mtext>J/K</mtext></mrow></mrow><mrow/></mrow><annotation encoding="StarMath 5.0"> size 12&#123;k=1 &quot;.&quot; &quot;38&quot; times &quot;10&quot; rSup &#123; size 8&#123; - &quot;23&quot;&#125; &#125; &quot;J/K&quot;&#125; &#123;&#125;</annotation></semantics></math> is Boltzmann’s constant, and <math xmlns:q="http://cnx.rice.edu/qml/1.0" xmlns:fo="urn:oasis:names:tc:opendocument:xmlns:xsl-fo-compatible:1.0" xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:md="http://cnx.rice.edu/mdml" xmlns="http://cnx.rice.edu/cnxml"><semantics><mrow><mrow><mrow><mtext>ln</mtext><mi>W</mi></mrow></mrow><mrow/></mrow><annotation encoding="StarMath 5.0"> size 12&#123;&quot;ln&quot; W&#125; &#123;&#125;</annotation></semantics></math> is the natural logarithm of the number of microstates <math xmlns:q="http://cnx.rice.edu/qml/1.0" xmlns:fo="urn:oasis:names:tc:opendocument:xmlns:xsl-fo-compatible:1.0" xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:md="http://cnx.rice.edu/mdml" xmlns="http://cnx.rice.edu/cnxml"><semantics><mrow><mrow><mi>W</mi></mrow><mrow/></mrow><annotation encoding="StarMath 5.0"> size 12&#123; W&#125; &#123;&#125;</annotation></semantics></math> corresponding to the given macrostate. <math xmlns:q="http://cnx.rice.edu/qml/1.0" xmlns:fo="urn:oasis:names:tc:opendocument:xmlns:xsl-fo-compatible:1.0" xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:md="http://cnx.rice.edu/mdml" xmlns="http://cnx.rice.edu/cnxml"><semantics><mrow><mrow><mi>W</mi></mrow><mrow/></mrow><annotation encoding="StarMath 5.0"> size 12&#123; W&#125; &#123;&#125;</annotation></semantics></math> is proportional to the probability that the macrostate will occur. Thus entropy is directly related to the probability of a state—the more likely the state, the greater its entropy. Boltzmann proved that this expression for <math xmlns:q="http://cnx.rice.edu/qml/1.0" xmlns:fo="urn:oasis:names:tc:opendocument:xmlns:xsl-fo-compatible:1.0" xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:md="http://cnx.rice.edu/mdml" xmlns="http://cnx.rice.edu/cnxml"><semantics><mrow><mrow><mi>S</mi></mrow><mrow/></mrow><annotation encoding="StarMath 5.0"> size 12&#123; S&#125; &#123;&#125;</annotation></semantics></math> is equivalent to the definition <math xmlns:q="http://cnx.rice.edu/qml/1.0" xmlns:fo="urn:oasis:names:tc:opendocument:xmlns:xsl-fo-compatible:1.0" xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:md="http://cnx.rice.edu/mdml" xmlns="http://cnx.rice.edu/cnxml"><semantics><mrow><mrow><mrow><mn>Δ</mn><mi>S</mi><mo stretchy="false">=</mo><mrow><mi>Q</mi><mo stretchy="false">/</mo><mi>T</mi></mrow></mrow></mrow><mrow/></mrow><annotation encoding="StarMath 5.0"> size 12&#123; ΔS=Q/T&#125; &#123;&#125;</annotation></semantics></math>, which we have used extensively.

Thus the second law of thermodynamics is explained on a very basic level: entropy either remains the same or increases in every process. This phenomenon is due to the extraordinarily small probability of a decrease, based on the extraordinarily larger number of microstates in systems with greater entropy. Entropy  *can* decrease, but for any macroscopic system, this outcome is so unlikely that it will never be observed.
