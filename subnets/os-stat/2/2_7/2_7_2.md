index: hide
name: Sampling Variability of a Statistic

The statistic of a sampling distribution was discussed in <link:>. How much the statistic varies from one sample to another is known as the   **sampling variability of a statistic**. You typically measure the sampling variability of a statistic by its standard error. The  *standard error of the mean* is an example of a standard error. It is a special standard deviation and is known as the standard deviation of the sampling distribution of the mean. You will cover the standard error of the mean in the chapter <link:> (not now). The notation for the standard error of the mean is <math xmlns:bib="http://bibtexml.sf.net/" xmlns:q="http://cnx.rice.edu/qml/1.0" xmlns:md="http://cnx.rice.edu/mdml" xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:cnxorg="http://cnx.rice.edu/system-info" xmlns="http://cnx.rice.edu/cnxml"> <mrow>  <mfrac>   <mi>σ</mi>   <mrow>    <msqrt>     <mi>n</mi>    </msqrt>   </mrow>  </mfrac> </mrow></math> where  *σ* is the standard deviation of the population and n is the size of the sample.

## Explanation of the standard deviation calculation shown in the table

The deviations show how spread out the data are about the mean. The data value 11.5 is farther from the mean than is the data value 11 which is indicated by the deviations 0.97 and 0.47. A positive deviation occurs when the data value is greater than the mean, whereas a negative deviation occurs when the data value is less than the mean. The deviation is –1.525 for the data value nine.  *If you add the deviations, the sum is always zero*. (For , there are  *n* = 20 deviations.) So you cannot simply add the deviations to get the spread of the data. By squaring the deviations, you make them positive numbers, and the sum will also be positive. The variance, then, is the average squared deviation.

The variance is a squared measure and does not have the same units as the data. Taking the square root solves the problem. The standard deviation measures the spread in the same units as the data.

Notice that instead of dividing by  *n* = 20, the calculation divided by  *n* – 1 = 20 – 1 = 19 because the data is a sample. For the  *sample* variance, we divide by the sample size minus one ( *n* – 1). Why not divide by  *n*?  The answer has to do with the population variance.  *The sample variance is an estimate of the population variance.* Based on the theoretical mathematics that lies behind these calculations, dividing by ( *n* – 1) gives a better estimate of the population variance.

The standard deviation,  *s* or  *σ*, is either zero or larger than zero. Describing the data with reference to the spread is called "variability".  The variability in data depends upon the method by which the outcomes are obtained; for example, by measuring or by random sampling. When the standard deviation is zero, there is no spread; that is, the all the data values are equal to each other. The standard deviation is small when the data are all concentrated close to the mean, and is larger when the data values show more variation from the mean. When the standard deviation is a lot larger than zero, the data values are very spread out about the mean; outliers can make  *s* or  *σ* very large.

The standard deviation, when first presented, can seem unclear.  By graphing your data, you can get a better "feel" for the deviations and the standard deviation.  You will find that in symmetrical distributions, the standard deviation can be very helpful but in skewed distributions, the standard deviation may not be much help. The reason is that the two sides of a skewed distribution have different spreads. In a skewed distribution, it is better to look at the first quartile, the median, the third quartile, the smallest value, and the largest value. Because numbers can be confusing,  *always graph your data*. Display your data in a histogram or a box plot.
