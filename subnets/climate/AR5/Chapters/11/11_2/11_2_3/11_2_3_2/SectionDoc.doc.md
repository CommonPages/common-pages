index: hide
public: ar5-
name: Section 11.2.3.2
title: 11.2.3.2 - Forecast Quality Assessment

The quality of a forecast system is assessed by estimating, among others, the accuracy, skill and reliability of a set of hindcasts ({cite.11.'Jolliffe_and_Stephenson_2011}). These three terms—accuracy, skill and reliability—are used here in a strict technical sense. A suite of measures needs to be considered, particularly when a forecast system are compared. The accuracy of a forecast system refers to the average distance/error between forecasts and observations. The skill score is a relative measure of the quality of the forecasting system compared to some benchmark or reference forecast (e.g., climatology or persistence). The reliability, which is a property of the specific forecast system, measures the trustworthiness of the predictions. Reliability measures how well the predicted probability distribution matches the observed relative frequency of the forecast event. Accuracy and reliability are aspects of forecast quality that can be improved by improving the individual forecast systems or by combining several of them into a multi-model prediction. The reliability can be improved by a posteriori corrections to {Glossary.*Model_spread model spread}. Forecast quality can also be improved by unequal weighting ({cite.11.'Weigel_et_al_2010}; {cite.11.'DelSole_et_al_2013}), although this option has not been explored in decadal prediction to date, because a long training sample is required to obtain robust weights.

The assessment of forecast quality depends on the quantities of greatest interest to those who use the information. World Meteorological Organization (WMO)’s Standard Verification System (SVS) for LongRange Forecasts (LRF) (WMO, 2002) outlines specifications for longrange (sub-seasonal to seasonal) forecast quality assessment. These measures are also described in {cite.11.'Jolliffe_and_Stephenson_2011 Jolliffe and Stephenson (2011)} and {cite.11.'Wilks_2006 Wilks (2006)}. A recommendation for a deterministic {Glossary.*Metric metric} for decadal {Topics.*Climate_Predictions climate predictions} is the mean square skill score (MSSS), and for a probabilistic metric, the continuous ranked probability skill score (CRPSS) as described in {cite.11.'Goddard_et_al_2013 Goddard et al. (2013)} and Meehl et al. (2013d). For dynamical {Glossary.*Ensemble ensemble} systems, a useful measure of the characteristics of an ensemble forecast system is spread. The relative spread can be described in terms of the ratio between the mean spread around the ensemble mean and the root mean square error (RMSE) of the ensemble-mean prediction, or spread-to-RMSE ratio. A ratio of 1 is considered a desirable feature for a Gaussian-distributed variable of a well-calibrated (i.e., reliable) prediction system ({cite.11.'Palmer_et_al_2006}). The importance of using statistical inference in forecast quality assessments has been recently emphasized (Garcia-Serrano and Doblas-Reyes, 2012; {cite.11.'Goddard_et_al_2013}). This is even more important when there are only small samples available ({cite.11.'Kumar_2009}) and a small number of degrees of freedom (Gangstø et al., 2013). Confidence intervals for the scores are typically computed using either parametric or bootstrap methods ({cite.11.'Lanzante_2005}; {cite.11.'Jolliffe_2007}; {cite.11.'Hanlon_et_al_2013}).

The skill of seasonal predictions can vary from generation to generation (Power et al. 1999) and from one generation of forecast systems to the next ({cite.11.'Balmaseda_et_al_1995}). This highlights the possibility that the skill of decadal predictions might also vary from one period to another. Certain initial conditions might precede more predictable near-term states than other initial conditions, and this has the potential to be reflected in predictive skill assessments. However, the short length of the period available to initialize and verify the predictions makes the analysis of the variations in skill very difficult.
