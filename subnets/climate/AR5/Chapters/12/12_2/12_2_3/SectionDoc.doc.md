index: hide
public: ar5-
name: Section 12.2.3
title: 12.2.3 - From Ensembles to Uncertainty Quantification

Ensembles like CMIP5 do not represent a systematically sampled family of models but rely on self-selection by the modelling groups. This opportunistic nature of MMEs has been discussed, for example, in {cite.12.'Tebaldi_and_Knutti_2007 Tebaldi and Knutti (2007)} and {cite.12.'Knutti_et_al_2010a Knutti et al. (2010a)}. These ensembles are therefore not designed to explore uncertainty in a coordinated manner, and the range of their results cannot be straightforwardly interpreted as an exhaustive range of plausible outcomes, even if some studies have shown how they appear to behave as well calibrated probabilistic forecasts for some large-scale quantities ({cite.12.'Annan_and_Hargreaves_2010}). Other studies have argued instead that the tail of distributions is by construction undersampled (Räisänen, 2007). In general, the difficulty in producing quantitative estimates of uncertainty based on multiple model output originates in their peculiarities as a statistical sample, neither random nor systematic, with possible dependencies among the members ({cite.12.'Jun_et_al_2008}; {cite.12.'Masson_and_Knutti_2011}; {cite.12.'Pennell_and_Reichler_2011}; {cite.12.'Knutti_et_al_2013}) and of spurious nature, that is, often counting among their members models with different degrees of complexities (different number of processes explicitly represented or parameterized) even within the category of {Glossary.*General_circulation general circulation} models.

Agreement between multiple models can be a {Glossary.*Source source} of information in an uncertainty assessment or confidence statement. Various methods have been proposed to indicate regions where models agree on the projected changes, agree on no change or disagree. Several of those methods are compared in {Box_12_1 Box 12.1}. Many figures use stippling or hatching to display such information, but it is important to note that confidence cannot be inferred from model agreement alone.

Perturbed physics experiments (PPEs) differ in their output interpretability for they can be, and have been, systematically constructed and as such lend themselves to a more straightforward treatment through statistical modelling ({cite.12.'Rougier_2007}; {cite.12.'Sanso_and_Forest_2009}). Uncertain parameters in a single model to whose values the output is known to be sensitive are targeted for perturbations. More often it is the parameters in the atmospheric component of the model that are varied ({cite.12.'Collins_et_al_2006a}; {cite.12.'Sanderson_et_al_2008}), and to date have in fact shown to be the source of the largest {Topics.*Uncertainty uncertainties} in large-scale response, but lately, with much larger computing power expense, also parameters within the ocean component have been perturbed ({cite.12.'Collins_et_al_2007}; {cite.12.'Brierley_et_al_2010}). Parameters in the land surface schemes have also been subject to perturbation studies ({cite.12.'Fischer_et_al_2011}; {cite.12.'Booth_et_al_2012}; {cite.12.'Lambert_et_al_2012}). Ranges of possible values are explored and often statistical models that fit the relationship between parameter values and model output, that is, emulators, are trained on the {Glossary.*Ensemble ensemble} and used to predict the outcome for unsampled parameter value combinations, in order to explore the parameter space more thoroughly that would otherwise be computationally affordable ({cite.12.'Rougier_et_al_2009}). The space of a single model simulations (even when filtered through observational constraints) can show a large range of outcomes for a given scenario ({cite.12.'Jackson_et_al_2008}). However, multi-model ensembles and perturbed physics ensembles produce modes and distributions of climate responses that can be different from one another, suggesting that one type of ensemble cannot be used as an analogue for the other ({cite.12.'Murphy_et_al_2007}; {cite.12.'Sanderson_et_al_2010}; {cite.12.'Yokohata_et_al_2010}; {cite.12.'Collins_et_al_2011}).

Many studies have made use of results from these ensembles to characterize uncertainty in future {Topics.*Climate_Projections projections}, and these will be assessed and their results incorporated when describing specific aspects of future climate responses. PPEs have been uniformly treated across the different studies through the statistical framework of analysis of computer experiments ({cite.12.'Sanso_et_al_2008}; {cite.12.'Rougier_et_al_2009}; {cite.12.'Harris_et_al_2010}) or, more plainly, as a thorough exploration of alternative responses reweighted by observational constraints ({cite.12.'Murphy_et_al_2004}; {cite.12.'Piani_et_al_2005}; {cite.12.'Forest_et_al_2008}; {cite.12.'Sexton_et_al_2012}). In all cases the construction of a probability distribution is facilitated by the systematic nature of the experiments. MMEs have generated a much more diversified treatment (1) according to the choice of applying weights to the different models on the basis of past performance or not ({cite.12.'Weigel_et_al_2010}) and (2) according to the choice between treating the different models and the truth as indistinguishable or treating each model as a version of the truth to which an error has been added ({cite.12.'Annan_and_Hargreaves_2010}; {cite.12.'Sanderson_and_Knutti_2012}). Many studies can be classified according to these two criteria and their combination, but even within each of the four resulting categories different studies produce different estimates of uncertainty, owing to the preponderance of a priori assumptions, explicitly in those studies that approach the problem through a Bayesian perspective, or only implicit in the choice of {Glossary.*Likelihood likelihood} models, or weighting. This makes the use of probabilistic and other results produced through statistical inference necessarily dependent on agreeing with a particular set of assumptions ({cite.12.'Sansom_et_al_2013}), given the lack of a full exploration of the robustness of probabilistic estimates to varying these assumptions.

In summary, there does not exist at present a single agreed on and robust formal methodology to deliver uncertainty quantification estimates of future changes in all climate variables (see also {Chapters.9.9_8.9_8_3 Section 9.8.3} and {cite.12.'Stephenson_et_al_2012}). As a consequence, in this chapter, statements using the calibrated uncertainty language are a result of the expert judgement of the authors, combining assessed literature results with an evaluation of models demonstrated ability (or lack thereof) in simulating the relevant processes (see {Chapters.9 Chapter 9}) and model consensus (or lack thereof) over future projections. In some cases when a significant relation is detected between model performance and reliability of its future projections, some models (or a particular parametric configuration) may be excluded (e.g., Arctic {Glossary.*Sea_ice sea ice}; {Chapters.12.12_4.12_4_6.12_4_6_1 Section 12.4.6.1} and {cite.12.'Joshi_et_al_2010}) but in general it remains an open research question to find significant connections of this kind that justify some form of weighting across the ensemble of models and produce aggregated future projections that are significantly different from straightforward one model–one vote ({cite.12.'Knutti_2010}) ensemble results. Therefore, most of the analyses performed for this chapter make use of all available models in the ensembles, with equal weight given to each of them unless otherwise stated.
