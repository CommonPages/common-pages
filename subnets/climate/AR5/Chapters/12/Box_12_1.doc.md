index: hide
public: ar5-Box-12.1
name: Box 12.1
title: Box 12.1 - Methods to Quantify Model Agreement in Maps

The climate change {Topics.*Climate_Projections projections} in this report are based on ensembles of {Topics.*Climate_Modelling climate models}. The {Glossary.*Ensemble ensemble} mean is a useful quantity to characterize the average response to {Topics.*Radiative_Forcing external forcings}, but does not convey any information on the robustness of this response across models, its uncertainty and/or {Glossary.*Likelihood likelihood} or its magnitude relative to unforced {Topics.*Climate_Variability climate variability}. In the IPCC {cite.1.'IPCC_2007 AR4} WGI contribution ({cite.12.'IPCC_2007}) several criteria were used to indicate robustness of change, most prominently in Figure SPM.7. In that figure, showing projected precipitation changes, stippling marked regions where at least 90% of the CMIP3 models agreed on the sign of the change. Regions where less than 66% of the models agreed on the sign were masked white. The resulting large white area was often misinterpreted as indicating large {Topics.*Uncertainty uncertainties} in the different models’ response to external forcings, but recent studies show that, for the most part, the disagreement in sign among models is found where projected changes are small and still within the modelled range of internal variability, that is, where a response to {Glossary.*Anthropogenic anthropogenic} forcings has not yet emerged locally in a statistically significant way ({cite.12.'Tebaldi_et_al_2011}; {cite.12.'Power_et_al_2012}).

A number of methods to indicate model robustness, involving an assessment of the significance of the change when compared to internal variability, have been proposed since AR4. The different methods share the purpose of identifying regions with large, significant or robust changes, regions with small changes, regions where models disagree or a combination of those. They do, however, use different assumptions about the statistical properties of the model ensemble, and therefore different criteria for synthesizing the information from it. Different methods also differ in the way they estimate internal variability. We briefly describe and compare several of these methods here.

Method (a): The default method used in Chapters {Chapters.11}, {Chapters.12} and {Chapters.14} as well as in the Annex I (hatching only) is shown in {Box_12_1 Box 12.1}, {'Box_12_1_Figure_1 Figure 1}a, and is based on relating the climate change signal to internal variability in 20-year means of the models as a reference3. Regions where the multi-model mean change exceeds two standard deviations of internal variability and where at least 90% of the models agree on the sign of change are stippled and interpreted as ‘large change with high model agreement’. Regions where the model mean is less than one standard deviation of internal variability are hatched and interpreted as ‘small signal or low agreement of models’. This can have various reasons: (1) changes in individual models are smaller than internal variability, or (2) although changes in individual models are significant, they disagree about the sign and the multi-model mean change remains small. Using this method, the case where all models scatter widely around zero and the case where all models agree on near zero change therefore are both hatched (e.g., precipitation change over the Amazon {Topics.*Regional_Climate region} by the end of the 21st century, which the following methods mark as ‘inconsistent model response’).

Method (b): Method (a) does not distinguish the case where all models agree on no change and the case where, for example, half of the models show a significant increase and half a decrease. The distinction may be relevant for many applications and a modification of method (a) is to restrict hatching to regions where there is high agreement among the models that the change will be ‘small’, thus eliminating the ambiguous interpretation ‘small or low agreement’ in (a). In contrast to method (a) where the model mean is compared to variability, this case (b) marks regions where at least 80% of the individual models show a change smaller than two standard deviations of variability with hatching. Grid points where many models show significant change but don’t agree are no longer hatched ({Box_12_1 Box 12.1}, {'Box_12_1_Figure_1 Figure 1}b).

Method (c): Knutti and Sedláček (2013) define a dimensionless robustness measure, R, which is inspired by the signal-to-noise ratio and the ranked probability skill score. It considers the natural variability and agreement on magnitude and sign of change. A value of R = 1 implies perfect model agreement; low or negative values imply poor model agreement (note that by definition R can assume any negative value). Any level of R can be chosen for the stippling. For illustration, in {Box_12_1 Box 12.1}, {'Box_12_1_Figure_1 Figure 1}c, regions with R > 0.8 are marked with small dots, regions with R > 0.9 with larger dots and are interpreted as ‘robust large change’. This yields similar results to method (a) for the end of the century, but with some areas of moderate model robustness (R > 0.8) already for the near-term projections, even though the signal is still within the noise. Regions where at least 80% of the models individually show no significant change are hatched and interpreted as ‘changes unlikely to emerge from variability’4.There is less hatching in this method than in method (a), because it requires 80% of the models to be within variability, not just the model average. Regions where at least 50% of the models show significant change but R< 0.5 are masked as white to indicate ‘models disagreeing on the projected change projections’ ({Box_12_1 Box 12.1}, {'Box_12_1_Figure_1 Figure 1}c).

Method (d): {cite.12.'Tebaldi_et_al_2011 Tebaldi et al. (2011)} start from IPCC AR4 SPM7 but separate lack of model agreement from lack of signal ({Box_12_1 Box 12.1}, {'Box_12_1_Figure_1 Figure 1}e). Grid points are stippled and interpreted as ‘robust large change’ when more than 50% of the models show significant change and at least 80% of those agree on the sign of change. Grid points where more than 50% of the models show significant change but less than 80% of those agree on the sign of change are masked as white and interpreted as ‘unreliable’. The results are again similar to the methods above. No hatching was defined in that method ({Box_12_1 Box 12.1} Figure 1d). (See also {cite.12.'Neelin_et_al_2006} for a similar approach applied to a specific regional domain.)

Method (e): {cite.12.'Power_et_al_2012 Power et al. (2012)} identify three distinct regions using various methods in which projections can be very loosely described as either: ‘statistically significant’, ‘small (relative to temporal variability) or zero, but not statistically significant’ or ‘uncertain’. The emphasis with this approach is to identify robust signals taking the models at face value and to address the questions: (1) What will change? (2) By how much? and (3) What will not change? The underlying consideration here is that statistical testing under the assumption of model independence provides a worthwhile, albeit imperfect, line of evidence that needs to be considered in conjunction with other evidence (e.g., degree of interdependence, ability of models to simulate the past), in order to assess the degree of confidence one has in a projected change.

The examples given here are not exhaustive but illustrate the main ideas. Other methods include simply counting the number of models agreeing on the sign ({cite.12.'Christensen_et_al_2007}), or varying colour hue and saturation to indicate magnitude of change and robustness of change separately ({cite.12.'Kaye_et_al_2012}). In summary, there are a variety of ways to characterize magnitude or significance of change, and agreement between models. There is also a compromise to make between clarity and richness of information. Different methods serve different purposes and a variety of criteria can be justified to highlight specific properties of multi-model ensembles. Clearly only a subset of information regarding robust and uncertain change can be conveyed in a single plot. The methods above convey some important pieces of this information, but obviously more information could be provided if more maps with additional statistics were provided. In fact Annex I provides more explicit information on the range of projected changes evident in the models (e.g., the median, and the upper and lower quartiles). For most of the methods there is a necessity to choose thresholds for the level of agreement that cannot be identified objectively, but could be the result of individual, application-specific evaluations. Note also that all of the above methods measure model agreement in an ensemble of opportunity, and it is impossible to derive a confidence or likelihood statement from the model agreement or {Glossary.*Model_spread model spread} alone, without considering consistency with observations, model dependence and the degree to which the relevant processes are understood and reflected in the models (see {Chapters.12.12_2.12_2_3 Section 12.2.3}).

The method used by {cite.12.'Power_et_al_2012 Power et al. (2012)} differs from the other methods in that it tests the statistical significance of the ensemble mean rather than a single simulation. As a result, the area where changes are significant increases with an increasing number of models. Already for the period centred on 2025, most of the grid points when using this method show significant change in the ensemble mean whereas in the other methods projections for this time period are classified as changes not exceeding internal variability. The reason is that the former produces a statement about the mean of the distribution being significantly different from zero, equivalent to treating the ensemble as ‘truth plus error’, that is, assuming that the models are independent and randomly distributed around reality. Methods a–d, on the other hand, use an ‘indistinguishable’ interpretation, in which each model and reality are drawn from the same distribution. In that case, the stippling and hatching characterize the likelihood of a single member being significant or not, rather than the ensemble mean. There is some debate in the literature on how the multi-model ensembles should be interpreted statistically. This and past IPCC reports treat the model spread as some measure of uncertainty, irrespective of the number of models, which implies an ‘indistinguishable’ interpretation. For a detailed discussion readers are referred to the literature ({cite.12.'Tebaldi_and_Knutti_2007}; {cite.12.'Annan_and_Hargreaves_2010}; {cite.12.'Knutti_et_al_2010a}, 2010b; {cite.12.'Annan_and_Hargreaves_2011a}; {cite.12.'Sanderson_and_Knutti_2012}).

{image:'Box_12_1_Figure_1}
