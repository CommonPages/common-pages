index: hide
public: ar5-
name: Section 9.2.2.3
title: 9.2.2.3 - Statistical Methods Applied to Ensembles

The most common approach to characterize MME results is to calculate the arithmetic mean of the individual model results, referred to as an unweighted multi-model mean. This approach of ‘one vote per model’ gives equal weight to each climate model regardless of (1) how many simulations each model has contributed, (2) how interdependent the models are or (3) how well each model has fared in objective evaluation. The multi-model mean will be used often in this chapter. Some {Topics.*Climate_Modelling climate models} share a common lineage and so share common biases ({cite.9.'Frame_et_al_2005}; {cite.9.'Tebaldi_and_Knutti_2007}; {cite.9.'Jun_et_al_2008}; {cite.9.'Knutti_2010}; {cite.9.'Knutti_et_al_2010a}, 2013; {cite.9.'Annan_and_Hargreaves_2011}; {cite.9.'Pennell_and_Reichler_2011}; Knutti and Sedlácek, 2013). As a result, collections such as the CMIP5 MME cannot be considered a random sample of independent models. This complexity creates challenges for how best to make quantitative inferences of future climate as discussed further in {Chapters.12 Chapter 12} ({cite.9.'Knutti_et_al_2010a}; {cite.9.'Collins_et_al_2012}; {cite.9.'Stephenson_et_al_2012}; {cite.9.'Sansom_et_al_2013}).

{cite.9.'Annan_and_Hargreaves_2010 Annan and Hargreaves (2010)} have proposed a ‘rank histogram’ approach to evaluate model ensembles as a whole, rather than individual models, by diagnosing whether observations can be considered statistically indistinguishable from a model {Glossary.*Ensemble ensemble}. Studies based on this approach have suggested that MMEs (CMIP3/5) are ‘reliable’ in that they are not too narrow or too dispersive as a sample of possible models, but existing single-model-based ensembles tend to be too narrow ({cite.9.'Yokohata_et_al_2012}, 2013). Although initial work has analysed the current mean climate state, further work is required to study the relationships between simulation errors and {Topics.*Uncertainty uncertainties} in ensembles of future {Topics.*Climate_Projections projections} ({cite.9.'Collins_et_al_2012}).

Bayesian methods offer insights into how to account for model inadequacies and combine information from several metrics in both MME and PPE approaches ({cite.9.'Sexton_and_Murphy_2012}; {cite.9.'Sexton_et_al_2012}), but they are complex. A simpler strategy of screening out some model variants on the basis of some observational comparison has been used with some PPEs ({cite.9.'Lambert_et_al_2012}; {cite.9.'Shiogama_et_al_2012}). {cite.9.'Edwards_et_al_2011 Edwards et al. (2011)} provided a statistical framework for ‘pre-calibrating’ out such poor model variants. Screening techniques have also been used with MMEs ({cite.9.'Santer_et_al_2009}).

Additional Bayesian methods are applied to the MMEs so that past model performance is combined with prior distributions to estimate uncertainty from the MME ({cite.9.'Furrer_et_al_2007}; {cite.9.'Tebaldi_and_Knutti_2007}; {cite.9.'Milliff_et_al_2011}). Similar to Bayesian PPE methods, common biases can be assessed within the MME to determine effective independence of the climate models ({cite.9.'Knutti_et_al_2013}) (see {Chapters.12.12_2.12_2_2 Section 12.2.2} for a discussion of the assumptions in the Bayesian approaches).
