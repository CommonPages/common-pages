index: hide
public: ar5-
name: Section 9.1.1
title: 9.1.1 - Scope and Overview of this Chapter

{Topics.*Climate_Modelling Climate models} are the primary tools available for investigating the response of the climate system to various {Topics.*Radiative_Forcing forcings}, for making {Topics.*Climate_Predictions climate predictions} on seasonal to decadal time scales and for making {Topics.*Climate_Projections projections of future climate} over the coming century and beyond. It is crucial therefore to evaluate the performance of these models, both individually and collectively. The focus of this chapter is primarily on the models whose results will be used in the {Glossary.*Detection_and_Attribution detection} and {Topics.*Attribution_of_Change attribution} {Chapters.10 Chapter 10} and the chapters that present and assess projections ({Chapters.11 Chapter 11} to 14; Annex I), and so this is necessarily an incomplete evaluation. In particular, this chapter draws heavily on model results collected as part of the Coupled {Topics.*Model_Intercomparison Model Intercomparison Projects} (CMIP3 and CMIP5) ({cite.9.'Meehl_et_al_2007}; {cite.9.'Taylor_et_al_2012b}), as these constitute a set of coordinated and thus consistent and increasingly well-documented climate model experiments. Other intercomparison efforts, such as those dealing with {Glossary.*Regional_Climate_Model Regional Climate Models} (RCMs) and those dealing with {Topics.*Climate_System Earth System} Models of Intermediate Complexity ({Glossary.*Earth_System_Model_of_Intermediate_Complexity EMICs}) are also used. It should be noted that the CMIP3 model archive has been extensively evaluated, and much of that evaluation has taken place subsequent to the {cite.1.'IPCC_2007 AR4}. By comparison, the CMIP5 models are only now being evaluated and so there is less published literature available. Where possible we show results from both CMIP3 and CMIP5 models so as to illustrate changes in model performance over time; however, where only CMIP3 results are available, they still constitute a useful evaluation of model performance in that for many quantities, the CMIP3 and CMIP5 model performances are broadly similar.

The direct approach to model evaluation is to compare model output with observations and analyze the resulting difference. This requires knowledge of the errors and {Topics.*Uncertainty uncertainties} in the observations, which have been discussed in {Chapters.2 Chapter 2} through 6. Where possible, averages over the same time period in both models and observations are compared, although for many quantities the observational record is rather short, or only observationally based estimates of the climatological mean are available. In cases where observations are lacking, we resort to intercomparison of model results to provide at least some quantification of model uncertainty via inter-{Glossary.*Model_spread model spread}.

After a more thorough discussion of the climate models and methods for evaluation in Sections {Chapters.9.9_1 9.1} and {Chapters.9.9_2 9.2}, we describe climate model experiments in {Chapters.9.9_3 Section 9.3}, evaluate recent and longer-term records as simulated by climate models in {Chapters.9.9_4 Section 9.4}, variability and extremes in {Chapters.9.9_5 Section 9.5}, and regional-scale climate simulation including {Glossary.*Downscaling downscaling} in {Chapters.9.9_6 Section 9.6}. We conclude with a discussion of model performance and {Glossary.*Climate_sensitivity climate sensitivity} in {Chapters.9.9_7 Section 9.7}, and the relation between model performance and the credibility of future climate projections in {Chapters.9.9_8 Section 9.8}.
