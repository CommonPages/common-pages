index: hide
public: ar5-
name: Section 9.4.1.3
title: 9.4.1.3 - Quantifying Model Performance with Metrics

Performance metrics were used to some extent in the Third Assessment Report ({cite.1.'IPCC_2001 TAR}) and the Fourth Assessment Report ({cite.1.'IPCC_2007 AR4}), and are expanded upon here because of their increased appearance in the recent literature. As a simple example, {'Figure_9_6 Figure 9.6} illustrates how the pattern correlation between the observed and simulated climatological annual mean spatial patterns depends very much on the quantity examined. All CMIP3 and CMIP5 models capture the mean {Topics.*Surface_Temperature surface temperature} distribution quite well, with correlations above 0.95, which are largely determined by the meridional temperature gradient. Correlations for outgoing {Glossary.*Longwave_radiation longwave radiation} are somewhat lower. For precipitation and the TOA shortwave cloud {Glossary.*Radiative_effect radiative effect}, the correlations between models and observations are below 0.90, and there is considerable scatter among model results. This example quantifies how some aspects of the simulated large-scale climate agree with observations better than others. Some of these differences are attributable to smoothly varying fields (e.g., temperature, water vapour) often agreeing better with observations than fields that exhibit fine structure (e.g., precipitation) (see also {Chapters.9.9_6.9_6_1.9_6_1_1 Section 9.6.1.1}). Incremental improvement in each field is also evident in {'Figure_9_6 Figure 9.6}, as gauged by the mean and median results in the CMIP5 {Glossary.*Ensemble ensemble} having higher correlations than CMIP3. This multi-variate quantification of model improvement across development cycles is evident in several studies (e.g., {cite.9.'Reichler_and_Kim_2008}; {cite.9.'Knutti_et_al_2013})

{image:'Figure_9_6}

{'Figure_9_7 Figure 9.7} (following {cite.9.'Gleckler_et_al_2008}) depicts the space–time rootmean-square error (RMSE) for the 1980–2005 climatological seasonal cycle of the historically forced CMIP5 simulations. For each of the fields examined, this ‘portrait plot’ depicts relative performance, with blue shading indicating performance being better, and red shading worse, than the median of all model results. In each case, two observations-based estimates are used to demonstrate the impact of the selection of reference data on the results. Some models consistently compare better with observations than others, some exhibit mixed performance and some stand out with relatively poor agreement with observations. For most fields, the choice of the observational data set does not substantially change the result for global error measures (e.g., between a state-of-the-art and an older-generation {Topics.*Reanalysis reanalysis}), indicating that inter-model differences are substantially larger than the differences between the two reference data sets or the impact of two different climatological periods (e.g., for radiation fields: Earth Radiation Budget Experiment (ERBE) 1984–1988; CERES EBAF, 2001–2011). Nevertheless, it is important to recognize that different data sets often rely on the same {Glossary.*Source source} of measurements, and that the results in this figure can have some sensitivity to a variety of factors such as instrument {Topics.*Uncertainty uncertainty}, sampling errors (e.g., limited record length of observations), the spatial scale of comparison, the domain considered and the choice of {Glossary.*Metric metric}.

{image:'Figure_9_7}

Another notable feature of {'Figure_9_7 Figure 9.7} is that in most cases the multi-model mean agrees more favourably with observations than any individual model. This has been long recognized to hold for {Glossary.*Surface_temperature surface temperature} and precipitation (e.g., {cite.9.'Lambert_and_Boer_2001}). However, since the AR4, it has become clear that this holds for a broad range of climatological fields ({cite.9.'Gleckler_et_al_2008}; {cite.9.'Pincus_et_al_2008}; {cite.9.'Knutti_et_al_2010a}) and is theoretically better understood ({cite.9.'Annan_and_Hargreaves_2011}). It is worth noting that when most models suffer from a common error, such as the cold bias at high latitudes in the upper {Glossary.*Troposphere troposphere} (see TA 200 hPa of {'Figure_9_7 Figure 9.7}), individual models can agree better with observations than the multi-model mean.

Correlations between the relative errors for different quantities in {'Figure_9_7 Figure 9.7} are known to exist, reflecting physical relationships in the model formulations and in the real world. Cluster analysis methods have recently been used in an attempt to reduce this redundancy (e.g., {cite.9.'Yokoi_et_al_2011}; {cite.9.'Nishii_et_al_2012}), thereby providing more succinct summaries of model performance. Some studies have attempted an overall skill score by averaging together the results from multiple metrics (e.g., {cite.9.'Reichler_and_Kim_2008}). Although this averaging process is largely arbitrary, combining the results of multiple metrics can reduce the chance that a poorer performing model will score well for the wrong reasons. Recent work ({cite.9.'Nishii_et_al_2012}) has demonstrated that different methods used to produce a multi-variate skill measure for the CMIP3 models did not substantially alter the conclusions about the better and lesser performing models.

Large scale performance metrics are a typical first-step toward quantifying model agreement with observations, and summarizing broad characteristics of model performance that are not focussed on a particular application. More specialized performance tests target aspects of a simulation believed to be especially important for constraining model {Topics.*Climate_Projections projections}, although to date the connections between particular performance metrics and reliability of future projections are not well established. This important topic is addressed in {Chapters.9.9_8.9_8_3 Section 9.8.3}, which highlights several identified relationships between model performance and {Glossary.*Projection projection} responses.
