index: hide

<div class="Citation">
    <div class="Citation-thumb CitationThumb-linked"  data-href="https://doi.org/10.3354/cr00932">
      <img src="https://static.claimspace.cloud/climate-study-static/refs/thumbs/9/Kjellstrom_et_al_2010-thumb.png" />
    </div>

  <div class="Citation-body">
    <div class="Citation-text">Kjellstrom et al., 2010: Daily and monthly temperature and precipitation statistics as performance indicators for regional climate models. <span class="Article-journal">Climate Research, </span><span class="Article-volume">44, </span>135-150pp.</div>
    <div class="Citation-links">
      <div class="CitationLink" data-href="https://doi.org/10.3354/cr00932">
        <div class="CitationLink-icon CitationLink-Doi"></div>
        <div class="CitationLink-text">DOI</div>
      </div>
      <div class="CitationLink" data-href="https://scholar.google.com/scholar?q=10.3354/cr00932">
        <div class="CitationLink-icon CitationLink-Scholar"></div>
        <div class="CitationLink-text">Google Scholar</div>
      </div>
    </div>
  </div>
</div>

We evaluated daily and monthly statistics of maximum and minimum temperatures and precipitation in an ensemble of 16 regional climate models (RCMs) forced by boundary conditions from reanalysis data for 1961â€“1990. A high-resolution gridded observational data set for land areas in Europe was used. Skill scores were calculated based on the match of simulated and observed empirical probability density functions. The evaluation for different variables, seasons and regions showed that some models were better/worse than others in an overall sense. It also showed that no model that was best/worst in all variables, seasons or regions. Biases in daily precipitation were most pronounced in the wettest part of the probability distribution where the RCMs tended to overestimate precipitation compared to observations. We also applied the skill scores as weights used to calculate weighted ensemble means of the variables. We found that weighted ensemble means were slightly better in comparison to observations than corresponding unweighted ensemble means for most seasons, regions and variables. A number of sensitivity tests showed that the weights were highly sensitive to the choice of skill score metric and data sets involved in the comparison.

<div class="Citation-copy">
&copy; Inter-Research Science Center, 2010
</div>